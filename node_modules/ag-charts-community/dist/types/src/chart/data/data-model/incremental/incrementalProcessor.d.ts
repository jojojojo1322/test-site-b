import { type GroupedData, type InternalDatumPropertyDefinition, type ProcessedData, type ScopeId } from '../../dataModelTypes';
import type { DataChangeDescription, DataSet } from '../../dataSet';
import type { DataModelContext } from '../dataModelContext';
import type { SpecializedProcessValueFn } from '../domain/processValueFactory';
import { ReducerManager } from '../reducers/reducerManager';
/**
 * Handles incremental reprocessing of data when DataSets change.
 *
 * INCREMENTAL REPROCESSING OPTIMIZATION:
 * Instead of reprocessing all data, we:
 * 1. Apply change descriptions to transform existing arrays
 * 2. Process only new insertions
 * 3. Update only affected domain bands
 * 4. Reuse existing group structures when possible
 * This can reduce processing time by 90%+ for small updates to large datasets
 */
export declare class IncrementalProcessor<D extends object, K extends keyof D & string> {
    private readonly ctx;
    private readonly reducerManager;
    constructor(ctx: DataModelContext<D, K>, reducerManager: ReducerManager);
    /**
     * Checks if incremental reprocessing is supported for the given data configuration.
     */
    isReprocessingSupported(processedData: ProcessedData<D>): boolean;
    /**
     * Performs incremental reprocessing of data based on change descriptions.
     */
    reprocessData(processedData: ProcessedData<D>, dataSets: Map<DataSet<any>, DataChangeDescription | undefined> | undefined, getProcessValue: (def: InternalDatumPropertyDefinition<K>) => SpecializedProcessValueFn, reprocessGroupProcessorsFn: (processedData: GroupedData<D>, scopeChanges: Map<ScopeId, DataChangeDescription>) => void, recomputeDomainsFn: (processedData: ProcessedData<D>) => void, collectOptimizationMetadataFn: (processedData: ProcessedData<D>, mode: 'reprocess') => void): ProcessedData<D>;
    /**
     * Updates banded domains based on pending changes.
     *
     * BANDING OPTIMIZATION:
     * - Divides large datasets into bands (default ~100 bands)
     * - Tracks which bands are "dirty" and need recalculation
     * - During updates, only dirty bands are reprocessed
     * - Significantly reduces domain calculation overhead for large datasets
     *
     * Example: 1M data points â†’ 100 bands of 10K points each
     * Adding 1000 points only dirties 1-2 bands instead of scanning all 1M points
     *
     * This optimizes domain recalculation by only marking affected bands as dirty.
     * Deduplicates change descriptions to avoid processing the same changes multiple times
     * when multiple scopes share the same DataSet.
     */
    private updateBandsForChanges;
    private reprocessBandedReducers;
    /**
     * Collects change descriptions from all DataSets before committing.
     */
    private collectScopeChanges;
    /**
     * Commits all pending transactions to the data arrays.
     * Deduplicates DataSets to avoid committing the same DataSet multiple times
     * when multiple scopes share the same DataSet.
     */
    private commitPendingTransactions;
    private buildDefinitionProcessors;
    /**
     * Pre-processes all insertions once per scope to avoid redundant computation.
     */
    private processAllInsertions;
    /**
     * Processes all updated items once per scope, adding them to the insertion cache.
     * This ensures updated values are available when transforming columns/keys arrays.
     */
    private processAllUpdates;
    /**
     * Processes all insertions for a given scope once, caching the results.
     * Returns a map from ADJUSTED destIndex to processed values for all keys and values.
     * The adjusted destIndex accounts for out-of-bounds insertions that need to be shifted.
     */
    private processInsertionsOnce;
    /**
     * Processes a single datum for the given scope, returning cached key/value results.
     * Shared between insert and update paths to keep behavior consistent.
     */
    private processDatum;
    /**
     * Generic utility to transform arrays using cached insertion results.
     * This reduces duplication across transformKeysArrays, transformColumnsArrays, and transformInvalidityArrays.
     */
    private transformArraysWithCache;
    /**
     * Transforms keys arrays using cached insertion results.
     */
    private transformKeysArrays;
    /**
     * Transforms columns arrays using cached insertion results.
     */
    private transformColumnsArrays;
    /**
     * Helper to transform a scope-based invalidity map.
     */
    private transformInvalidityMap;
    /**
     * Transforms invalidity arrays using cached insertion results.
     */
    private transformInvalidityArrays;
    /**
     * Applies a change description to an array using the provided cache-aware extractor.
     * Shared by array transformation helpers to keep update logic consistent.
     */
    private applyChangeDescWithCache;
    /**
     * Transforms the groups array for grouped data during reprocessing.
     * Only called when groupsUnique=true and no invalid keys exist.
     *
     * This maintains the invariant: groups[i] corresponds to datum at columns[i].
     */
    private transformGroupsArray;
    /**
     * Creates a new DataGroup for an inserted datum during reprocessing.
     *
     * When groupsUnique=true and no invalid keys exist, each datum has:
     * - A unique set of keys
     * - datumIndices[columnIdx] = [0] (relative offset is always 0)
     * - All scopes are valid initially (unless invalid value detected)
     */
    private createDataGroupForInsertion;
    /**
     * Generates diff metadata for animations and incremental rendering.
     * This is an opt-in feature - only runs if diff tracking is already initialized.
     */
    private generateDiffMetadata;
    /**
     * Updates metadata after array transformations.
     * Uses intelligent cache management based on change patterns.
     */
    private updateProcessedDataMetadata;
    /**
     * Updates sort order entry incrementally for appended values.
     * Checks if new values maintain the existing ordering/uniqueness.
     */
    private updateSortOrderForAppend;
    /**
     * Updates KEY_SORT_ORDERS incrementally after an append operation.
     */
    private updateKeySortOrdersForAppend;
    /**
     * Invalidates sort order metadata BEFORE domain recomputation.
     *
     * This must be called BEFORE recomputeDomains() so that BandedDomain.setSortOrderMetadata()
     * receives the correct (possibly cleared) metadata. Without this, rolling window operations
     * would see stale sort order data and incorrectly configure sub-domains for sorted mode.
     *
     * @param anyKeyChanged - Whether any key values changed during update processing
     */
    private invalidateSortOrdersForChanges;
    /**
     * Updates KEY_SORT_ORDERS incrementally after a rolling window operation.
     * Rolling window = contiguous removals at start + appends at end.
     */
    private updateKeySortOrdersForRollingWindow;
    /**
     * Invalidates domain range caches after domain recomputation.
     *
     * Called AFTER recomputeDomains() to mark domain ranges as dirty for lazy rebuild.
     * Sort order invalidation is handled separately by invalidateSortOrdersForChanges().
     */
    private invalidateCachesForChanges;
    /**
     * Marks all RangeLookup entries as dirty for lazy rebuild.
     */
    private markDomainRangesDirty;
    /**
     * Recounts invalid entries for the given map into the provided counts map.
     */
    private recountInvalid;
    /**
     * Recomputes processor outputs using their incrementalCalculate hook when available.
     * Falls back to calculate to avoid stale reducer outputs if a processor lacks the hook.
     */
    private reprocessProcessors;
    /**
     * Helper to get unique DataSets from processed data.
     */
    private getUniqueDataSets;
}
